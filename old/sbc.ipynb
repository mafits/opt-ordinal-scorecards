{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBC (single binary classifier) reduction and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "X = pd.DataFrame(X)\n",
    "y = np.array(['a', 'b', 'c', 'd'])\n",
    "y = pd.Series(y)\n",
    "\n",
    "mapping = {0: 'a', 1: 'b', 2: 'c', 3: 'd'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subjective Evaluation</th>\n",
       "      <th>BRA Value</th>\n",
       "      <th>LBC Value</th>\n",
       "      <th>UNR Value</th>\n",
       "      <th>BCE Value</th>\n",
       "      <th>BCD Value</th>\n",
       "      <th>BAD Value</th>\n",
       "      <th>BOD Value</th>\n",
       "      <th>pBRA Value</th>\n",
       "      <th>pLBC Value</th>\n",
       "      <th>...</th>\n",
       "      <th>pBAD Value</th>\n",
       "      <th>pBOD Value</th>\n",
       "      <th>cX2L Value</th>\n",
       "      <th>cX2a Value</th>\n",
       "      <th>cX2b Value</th>\n",
       "      <th>cX2Lab Value</th>\n",
       "      <th>cEMDL Value</th>\n",
       "      <th>cEMDa Value</th>\n",
       "      <th>cEMDb Value</th>\n",
       "      <th>cEMDLab Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good</td>\n",
       "      <td>1.149009</td>\n",
       "      <td>0.830915</td>\n",
       "      <td>0.107618</td>\n",
       "      <td>0.938534</td>\n",
       "      <td>1.934477</td>\n",
       "      <td>4.885198</td>\n",
       "      <td>17.29637</td>\n",
       "      <td>0.049401</td>\n",
       "      <td>0.033356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029188</td>\n",
       "      <td>0.10334</td>\n",
       "      <td>0.097502</td>\n",
       "      <td>0.009506</td>\n",
       "      <td>0.217169</td>\n",
       "      <td>0.10067</td>\n",
       "      <td>1.503086</td>\n",
       "      <td>0.030362</td>\n",
       "      <td>0.291363</td>\n",
       "      <td>0.210082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>2.368824</td>\n",
       "      <td>1.326447</td>\n",
       "      <td>1.599444</td>\n",
       "      <td>0.272997</td>\n",
       "      <td>0.035378</td>\n",
       "      <td>19.932998</td>\n",
       "      <td>68.761479</td>\n",
       "      <td>0.113919</td>\n",
       "      <td>0.058509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117875</td>\n",
       "      <td>0.406627</td>\n",
       "      <td>0.305692</td>\n",
       "      <td>0.159653</td>\n",
       "      <td>0.193317</td>\n",
       "      <td>0.061736</td>\n",
       "      <td>1.2486</td>\n",
       "      <td>0.206529</td>\n",
       "      <td>0.351693</td>\n",
       "      <td>0.105346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good</td>\n",
       "      <td>1.409939</td>\n",
       "      <td>0.950935</td>\n",
       "      <td>0.278358</td>\n",
       "      <td>1.229293</td>\n",
       "      <td>2.221074</td>\n",
       "      <td>22.27777</td>\n",
       "      <td>38.32343</td>\n",
       "      <td>0.067231</td>\n",
       "      <td>0.038613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120132</td>\n",
       "      <td>0.206658</td>\n",
       "      <td>0.109386</td>\n",
       "      <td>0.024217</td>\n",
       "      <td>0.280625</td>\n",
       "      <td>0.012974</td>\n",
       "      <td>0.545858</td>\n",
       "      <td>0.07552</td>\n",
       "      <td>0.304185</td>\n",
       "      <td>0.049345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>2.059918</td>\n",
       "      <td>0.720926</td>\n",
       "      <td>1.786307</td>\n",
       "      <td>1.065381</td>\n",
       "      <td>1.995007</td>\n",
       "      <td>25.16341</td>\n",
       "      <td>31.63107</td>\n",
       "      <td>0.100705</td>\n",
       "      <td>0.032141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15603</td>\n",
       "      <td>0.196133</td>\n",
       "      <td>0.038581</td>\n",
       "      <td>0.032267</td>\n",
       "      <td>0.026153</td>\n",
       "      <td>0.010779</td>\n",
       "      <td>0.334111</td>\n",
       "      <td>0.105626</td>\n",
       "      <td>0.137693</td>\n",
       "      <td>0.031455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good</td>\n",
       "      <td>2.452595</td>\n",
       "      <td>0.798596</td>\n",
       "      <td>1.868745</td>\n",
       "      <td>1.070148</td>\n",
       "      <td>1.218189</td>\n",
       "      <td>0.395585</td>\n",
       "      <td>31.426783</td>\n",
       "      <td>0.117249</td>\n",
       "      <td>0.035235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00282</td>\n",
       "      <td>0.224062</td>\n",
       "      <td>0.021259</td>\n",
       "      <td>0.027071</td>\n",
       "      <td>0.029206</td>\n",
       "      <td>0.006495</td>\n",
       "      <td>0.165401</td>\n",
       "      <td>0.081549</td>\n",
       "      <td>0.091768</td>\n",
       "      <td>0.030546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Excellent</td>\n",
       "      <td>0.951696</td>\n",
       "      <td>0.113046</td>\n",
       "      <td>0.950776</td>\n",
       "      <td>1.063822</td>\n",
       "      <td>1.757753</td>\n",
       "      <td>8.532637</td>\n",
       "      <td>17.880099</td>\n",
       "      <td>0.044664</td>\n",
       "      <td>0.004796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056474</td>\n",
       "      <td>0.11834</td>\n",
       "      <td>0.084273</td>\n",
       "      <td>0.042964</td>\n",
       "      <td>0.128076</td>\n",
       "      <td>0.037557</td>\n",
       "      <td>0.775643</td>\n",
       "      <td>0.162126</td>\n",
       "      <td>0.209275</td>\n",
       "      <td>0.105691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fair</td>\n",
       "      <td>0.895108</td>\n",
       "      <td>0.850773</td>\n",
       "      <td>0.453674</td>\n",
       "      <td>0.3971</td>\n",
       "      <td>0.559489</td>\n",
       "      <td>0.471835</td>\n",
       "      <td>16.884602</td>\n",
       "      <td>0.042506</td>\n",
       "      <td>0.036583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>0.127797</td>\n",
       "      <td>0.042711</td>\n",
       "      <td>0.02955</td>\n",
       "      <td>0.045899</td>\n",
       "      <td>0.034033</td>\n",
       "      <td>0.179981</td>\n",
       "      <td>0.086489</td>\n",
       "      <td>0.147702</td>\n",
       "      <td>0.050502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Good</td>\n",
       "      <td>3.100714</td>\n",
       "      <td>1.614997</td>\n",
       "      <td>2.713865</td>\n",
       "      <td>1.098868</td>\n",
       "      <td>0.498283</td>\n",
       "      <td>12.972883</td>\n",
       "      <td>33.147592</td>\n",
       "      <td>0.138786</td>\n",
       "      <td>0.070177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081306</td>\n",
       "      <td>0.207749</td>\n",
       "      <td>0.084506</td>\n",
       "      <td>0.083272</td>\n",
       "      <td>0.644688</td>\n",
       "      <td>0.131429</td>\n",
       "      <td>1.235154</td>\n",
       "      <td>0.137911</td>\n",
       "      <td>0.629805</td>\n",
       "      <td>0.234235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Good</td>\n",
       "      <td>0.978128</td>\n",
       "      <td>0.006449</td>\n",
       "      <td>0.515795</td>\n",
       "      <td>0.522243</td>\n",
       "      <td>3.378563</td>\n",
       "      <td>22.43618</td>\n",
       "      <td>33.791902</td>\n",
       "      <td>0.044836</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155069</td>\n",
       "      <td>0.233554</td>\n",
       "      <td>0.088175</td>\n",
       "      <td>0.058499</td>\n",
       "      <td>0.432515</td>\n",
       "      <td>0.012727</td>\n",
       "      <td>0.509642</td>\n",
       "      <td>0.125125</td>\n",
       "      <td>0.397542</td>\n",
       "      <td>0.025845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Poor</td>\n",
       "      <td>4.287771</td>\n",
       "      <td>2.025147</td>\n",
       "      <td>4.080698</td>\n",
       "      <td>2.055551</td>\n",
       "      <td>5.83788</td>\n",
       "      <td>79.573665</td>\n",
       "      <td>95.960856</td>\n",
       "      <td>0.18933</td>\n",
       "      <td>0.081771</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420374</td>\n",
       "      <td>0.506945</td>\n",
       "      <td>0.282464</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.433739</td>\n",
       "      <td>0.119059</td>\n",
       "      <td>1.540188</td>\n",
       "      <td>0.074895</td>\n",
       "      <td>0.410333</td>\n",
       "      <td>0.200164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Subjective Evaluation BRA Value LBC Value UNR Value BCE Value BCD Value  \\\n",
       "0                  Good  1.149009  0.830915  0.107618  0.938534  1.934477   \n",
       "1                  Good  2.368824  1.326447  1.599444  0.272997  0.035378   \n",
       "2                  Good  1.409939  0.950935  0.278358  1.229293  2.221074   \n",
       "3                  Good  2.059918  0.720926  1.786307  1.065381  1.995007   \n",
       "4                  Good  2.452595  0.798596  1.868745  1.070148  1.218189   \n",
       "5             Excellent  0.951696  0.113046  0.950776  1.063822  1.757753   \n",
       "6                  Fair  0.895108  0.850773  0.453674    0.3971  0.559489   \n",
       "7                  Good  3.100714  1.614997  2.713865  1.098868  0.498283   \n",
       "8                  Good  0.978128  0.006449  0.515795  0.522243  3.378563   \n",
       "9                  Poor  4.287771  2.025147  4.080698  2.055551   5.83788   \n",
       "\n",
       "   BAD Value  BOD Value pBRA Value pLBC Value  ... pBAD Value pBOD Value  \\\n",
       "0   4.885198   17.29637   0.049401   0.033356  ...   0.029188    0.10334   \n",
       "1  19.932998  68.761479   0.113919   0.058509  ...   0.117875   0.406627   \n",
       "2   22.27777   38.32343   0.067231   0.038613  ...   0.120132   0.206658   \n",
       "3   25.16341   31.63107   0.100705   0.032141  ...    0.15603   0.196133   \n",
       "4   0.395585  31.426783   0.117249   0.035235  ...    0.00282   0.224062   \n",
       "5   8.532637  17.880099   0.044664   0.004796  ...   0.056474    0.11834   \n",
       "6   0.471835  16.884602   0.042506   0.036583  ...   0.003571   0.127797   \n",
       "7  12.972883  33.147592   0.138786   0.070177  ...   0.081306   0.207749   \n",
       "8   22.43618  33.791902   0.044836   0.000262  ...   0.155069   0.233554   \n",
       "9  79.573665  95.960856    0.18933   0.081771  ...   0.420374   0.506945   \n",
       "\n",
       "  cX2L Value cX2a Value cX2b Value cX2Lab Value cEMDL Value cEMDa Value  \\\n",
       "0   0.097502   0.009506   0.217169      0.10067    1.503086    0.030362   \n",
       "1   0.305692   0.159653   0.193317     0.061736      1.2486    0.206529   \n",
       "2   0.109386   0.024217   0.280625     0.012974    0.545858     0.07552   \n",
       "3   0.038581   0.032267   0.026153     0.010779    0.334111    0.105626   \n",
       "4   0.021259   0.027071   0.029206     0.006495    0.165401    0.081549   \n",
       "5   0.084273   0.042964   0.128076     0.037557    0.775643    0.162126   \n",
       "6   0.042711    0.02955   0.045899     0.034033    0.179981    0.086489   \n",
       "7   0.084506   0.083272   0.644688     0.131429    1.235154    0.137911   \n",
       "8   0.088175   0.058499   0.432515     0.012727    0.509642    0.125125   \n",
       "9   0.282464   0.028169   0.433739     0.119059    1.540188    0.074895   \n",
       "\n",
       "  cEMDb Value cEMDLab Value  \n",
       "0    0.291363      0.210082  \n",
       "1    0.351693      0.105346  \n",
       "2    0.304185      0.049345  \n",
       "3    0.137693      0.031455  \n",
       "4    0.091768      0.030546  \n",
       "5    0.209275      0.105691  \n",
       "6    0.147702      0.050502  \n",
       "7    0.629805      0.234235  \n",
       "8    0.397542      0.025845  \n",
       "9    0.410333      0.200164  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aesthetic_evaluation_data = pd.read_csv('../datasets/aesthetic_evaluation_data.csv').drop(columns=['Image Filename','Author','Objective Evaluation', 'sX2L Value','sX2a Value','sX2b Value','sX2Lab Value','sEMDL Value','sEMDa Value','sEMDb Value','sEMDLab Value'])\n",
    "\n",
    "# get only the first 10 rows \n",
    "# but since there's no 'Poor' in the first 10 rows, add it manually\n",
    "poor_row = aesthetic_evaluation_data[aesthetic_evaluation_data['Subjective Evaluation'] == 'Poor'].iloc[0]\n",
    "aesthetic_evaluation_data = aesthetic_evaluation_data.head(9)\n",
    "aesthetic_evaluation_data = pd.concat([aesthetic_evaluation_data, poor_row.to_frame().T], ignore_index=True)\n",
    "\n",
    "aesthetic_evaluation_X = aesthetic_evaluation_data.drop(columns='Subjective Evaluation')\n",
    "aesthetic_evaluation_y = aesthetic_evaluation_data['Subjective Evaluation']\n",
    "\n",
    "aesthetic_evaluation_mapping = {0: 'Poor', 1: 'Fair', 2: 'Good', 3: 'Excellent'}\n",
    "\n",
    "aesthetic_evaluation_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for $K$ classes\n",
    "- find $K-1$ parallel hyperplanes.\n",
    "- $s$ is the number of classes to the left / right of the hyperplane.\n",
    "- for $s = K-1$,\n",
    "- an hyperplane separates 2 sets of classes.\n",
    "- each hyperplane is a binary classification problem, with the additional constraint of parallelism.\n",
    "- all these problems have to be solved simultaneously in an augmented feature space\n",
    "    - for $x$ in $R^{d}$, the augmented feature space is $R^{d+1}$\n",
    "- for each $x$ create $(K-1)$ new points: $(x, 0)(x,h_{1})...(x,h_{K-2})$, where $h$ is a positive constant\n",
    "- define a binary training set\n",
    "    - $(x_i^{(1)},0)$ belongs to $B_1$ and $(x_i^{(2)},0)..(x_i^{(K)},0)$ belong to $B_2$\n",
    "    - $(x_i^{(1)},h_1)$ and $(x_i^{(2)},h_1)$ belongs to $B_1$ and $(x_i^{(3)},h_1)..(x_i^{(K)}, h_1)$ belong to $B_2$\n",
    "    - ...\n",
    "    - $(x_i^{(1)},h_{K-2})..(x_i^{(K-1)},h_{K-2})$ belong to $B_1$ and $(x_i^{(K)},h_{K-2})$ belongs to $B_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original num classes:  4\n",
      "original num observations:  4\n",
      "using provided mapping:  {0: 'a', 1: 'b', 2: 'c', 3: 'd'}\n",
      "new num classes:  2\n",
      "new num observations:  12  (original num observations * 3 )\n",
      "old data:\n",
      "   0  1  0\n",
      "0  1  2  a\n",
      "1  3  4  b\n",
      "2  5  6  c\n",
      "3  7  8  d\n",
      "new data:\n",
      "    0  1  2  binary_label\n",
      "0   1  2  0             0\n",
      "1   1  2  1             0\n",
      "2   1  2  2             0\n",
      "3   3  4  0             1\n",
      "4   3  4  1             0\n",
      "5   3  4  2             0\n",
      "6   5  6  0             1\n",
      "7   5  6  1             1\n",
      "8   5  6  2             0\n",
      "9   7  8  0             1\n",
      "10  7  8  1             1\n",
      "11  7  8  2             1\n"
     ]
    }
   ],
   "source": [
    "def sbc_reduction(X, y, mapping=None, h=1):\n",
    "    # num of classes\n",
    "    K = len(np.unique(y))\n",
    "    \n",
    "    print(\"original num classes: \", K)\n",
    "    print(\"original num observations: \", X.shape[0])\n",
    "    \n",
    "    # num of parallel hyperplanes to be created (and replicas)\n",
    "    s = K-1\n",
    "    \n",
    "    # if class labels not integer, convert to integer\n",
    "    if not np.issubdtype(y.dtype, np.integer):\n",
    "        # if a mapping is not provided, create one\n",
    "        if mapping is None:\n",
    "            new_y = pd.Series(pd.factorize(y)[0])\n",
    "            # show the mapping\n",
    "            mapping = dict(enumerate(pd.factorize(y)[1]))\n",
    "            print(\"mapping: \", mapping)\n",
    "            y = new_y\n",
    "        # if it is, use it\n",
    "        else:\n",
    "            print(\"using provided mapping: \", mapping)\n",
    "            y = pd.Series(y.map(lambda v: {v_:k_ for k_, v_ in mapping.items()}[v]))\n",
    "\n",
    "    \n",
    "    # for each point, create s replicas each with a new feature in [0, h, h*2, ... h*(s-1)]\n",
    "    # the new label is a binary label\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "    for i in range(X.shape[0]): # for each point\n",
    "        for j in range(s): # for each replica\n",
    "            new_X.append(np.append(X.iloc[i].values, h*j))\n",
    "            new_label = y.iloc[i] > j\n",
    "            new_y.append(new_label.astype(int))\n",
    "    \n",
    "    new_X = pd.DataFrame(new_X).reset_index(drop=True)\n",
    "    new_y = pd.DataFrame(new_y).reset_index(drop=True)\n",
    "    new_data = pd.concat([new_X, new_y], axis=1)\n",
    "    # rename binary label column\n",
    "    new_data.columns = list(new_X.columns) + ['binary_label']\n",
    "    \n",
    "    print(\"new num classes: \", len(np.unique(new_y)))\n",
    "    print(\"new num observations: \", new_X.shape[0], \" (original num observations *\", s, \")\")\n",
    "    \n",
    "    return new_X, new_y, new_data, mapping\n",
    "\n",
    "\n",
    "new_X, new_y, new_data, mapping = sbc_reduction(X, y, mapping)\n",
    "old_data = pd.concat([X, y], axis=1)\n",
    "print(\"old data:\")\n",
    "print(old_data)\n",
    "print(\"new data:\")\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original num classes:  4\n",
      "original num observations:  10\n",
      "using provided mapping:  {0: 'Poor', 1: 'Fair', 2: 'Good', 3: 'Excellent'}\n",
      "new num classes:  2\n",
      "new num observations:  30  (original num observations * 3 )\n",
      "old aesthetic evaluation data:\n",
      "  Subjective Evaluation BRA Value LBC Value UNR Value BCE Value BCD Value  \\\n",
      "0                  Good  1.149009  0.830915  0.107618  0.938534  1.934477   \n",
      "1                  Good  2.368824  1.326447  1.599444  0.272997  0.035378   \n",
      "2                  Good  1.409939  0.950935  0.278358  1.229293  2.221074   \n",
      "3                  Good  2.059918  0.720926  1.786307  1.065381  1.995007   \n",
      "4                  Good  2.452595  0.798596  1.868745  1.070148  1.218189   \n",
      "5             Excellent  0.951696  0.113046  0.950776  1.063822  1.757753   \n",
      "6                  Fair  0.895108  0.850773  0.453674    0.3971  0.559489   \n",
      "7                  Good  3.100714  1.614997  2.713865  1.098868  0.498283   \n",
      "8                  Good  0.978128  0.006449  0.515795  0.522243  3.378563   \n",
      "9                  Poor  4.287771  2.025147  4.080698  2.055551   5.83788   \n",
      "\n",
      "   BAD Value  BOD Value pBRA Value pLBC Value  ... pBAD Value pBOD Value  \\\n",
      "0   4.885198   17.29637   0.049401   0.033356  ...   0.029188    0.10334   \n",
      "1  19.932998  68.761479   0.113919   0.058509  ...   0.117875   0.406627   \n",
      "2   22.27777   38.32343   0.067231   0.038613  ...   0.120132   0.206658   \n",
      "3   25.16341   31.63107   0.100705   0.032141  ...    0.15603   0.196133   \n",
      "4   0.395585  31.426783   0.117249   0.035235  ...    0.00282   0.224062   \n",
      "5   8.532637  17.880099   0.044664   0.004796  ...   0.056474    0.11834   \n",
      "6   0.471835  16.884602   0.042506   0.036583  ...   0.003571   0.127797   \n",
      "7  12.972883  33.147592   0.138786   0.070177  ...   0.081306   0.207749   \n",
      "8   22.43618  33.791902   0.044836   0.000262  ...   0.155069   0.233554   \n",
      "9  79.573665  95.960856    0.18933   0.081771  ...   0.420374   0.506945   \n",
      "\n",
      "  cX2L Value cX2a Value cX2b Value cX2Lab Value cEMDL Value cEMDa Value  \\\n",
      "0   0.097502   0.009506   0.217169      0.10067    1.503086    0.030362   \n",
      "1   0.305692   0.159653   0.193317     0.061736      1.2486    0.206529   \n",
      "2   0.109386   0.024217   0.280625     0.012974    0.545858     0.07552   \n",
      "3   0.038581   0.032267   0.026153     0.010779    0.334111    0.105626   \n",
      "4   0.021259   0.027071   0.029206     0.006495    0.165401    0.081549   \n",
      "5   0.084273   0.042964   0.128076     0.037557    0.775643    0.162126   \n",
      "6   0.042711    0.02955   0.045899     0.034033    0.179981    0.086489   \n",
      "7   0.084506   0.083272   0.644688     0.131429    1.235154    0.137911   \n",
      "8   0.088175   0.058499   0.432515     0.012727    0.509642    0.125125   \n",
      "9   0.282464   0.028169   0.433739     0.119059    1.540188    0.074895   \n",
      "\n",
      "  cEMDb Value cEMDLab Value  \n",
      "0    0.291363      0.210082  \n",
      "1    0.351693      0.105346  \n",
      "2    0.304185      0.049345  \n",
      "3    0.137693      0.031455  \n",
      "4    0.091768      0.030546  \n",
      "5    0.209275      0.105691  \n",
      "6    0.147702      0.050502  \n",
      "7    0.629805      0.234235  \n",
      "8    0.397542      0.025845  \n",
      "9    0.410333      0.200164  \n",
      "\n",
      "[10 rows x 23 columns]\n",
      "new aesthetic evaluation data:\n",
      "           0         1         2         3         4          5          6  \\\n",
      "0   1.149009  0.830915  0.107618  0.938534  1.934477   4.885198  17.296370   \n",
      "1   1.149009  0.830915  0.107618  0.938534  1.934477   4.885198  17.296370   \n",
      "2   1.149009  0.830915  0.107618  0.938534  1.934477   4.885198  17.296370   \n",
      "3   2.368824  1.326447  1.599444  0.272997  0.035378  19.932998  68.761479   \n",
      "4   2.368824  1.326447  1.599444  0.272997  0.035378  19.932998  68.761479   \n",
      "5   2.368824  1.326447  1.599444  0.272997  0.035378  19.932998  68.761479   \n",
      "6   1.409939  0.950935  0.278358  1.229293  2.221074  22.277770  38.323430   \n",
      "7   1.409939  0.950935  0.278358  1.229293  2.221074  22.277770  38.323430   \n",
      "8   1.409939  0.950935  0.278358  1.229293  2.221074  22.277770  38.323430   \n",
      "9   2.059918  0.720926  1.786307  1.065381  1.995007  25.163410  31.631070   \n",
      "10  2.059918  0.720926  1.786307  1.065381  1.995007  25.163410  31.631070   \n",
      "11  2.059918  0.720926  1.786307  1.065381  1.995007  25.163410  31.631070   \n",
      "12  2.452595  0.798596  1.868745  1.070148  1.218189   0.395585  31.426783   \n",
      "13  2.452595  0.798596  1.868745  1.070148  1.218189   0.395585  31.426783   \n",
      "14  2.452595  0.798596  1.868745  1.070148  1.218189   0.395585  31.426783   \n",
      "15  0.951696  0.113046  0.950776  1.063822  1.757753   8.532637  17.880099   \n",
      "16  0.951696  0.113046  0.950776  1.063822  1.757753   8.532637  17.880099   \n",
      "17  0.951696  0.113046  0.950776  1.063822  1.757753   8.532637  17.880099   \n",
      "18  0.895108  0.850773  0.453674  0.397100  0.559489   0.471835  16.884602   \n",
      "19  0.895108  0.850773  0.453674  0.397100  0.559489   0.471835  16.884602   \n",
      "20  0.895108  0.850773  0.453674  0.397100  0.559489   0.471835  16.884602   \n",
      "21  3.100714  1.614997  2.713865  1.098868  0.498283  12.972883  33.147592   \n",
      "22  3.100714  1.614997  2.713865  1.098868  0.498283  12.972883  33.147592   \n",
      "23  3.100714  1.614997  2.713865  1.098868  0.498283  12.972883  33.147592   \n",
      "24  0.978128  0.006449  0.515795  0.522243  3.378563  22.436180  33.791902   \n",
      "25  0.978128  0.006449  0.515795  0.522243  3.378563  22.436180  33.791902   \n",
      "26  0.978128  0.006449  0.515795  0.522243  3.378563  22.436180  33.791902   \n",
      "27  4.287771  2.025147  4.080698  2.055551  5.837880  79.573665  95.960856   \n",
      "28  4.287771  2.025147  4.080698  2.055551  5.837880  79.573665  95.960856   \n",
      "29  4.287771  2.025147  4.080698  2.055551  5.837880  79.573665  95.960856   \n",
      "\n",
      "           7         8         9  ...        14        15        16        17  \\\n",
      "0   0.049401  0.033356  0.005522  ...  0.097502  0.009506  0.217169  0.100670   \n",
      "1   0.049401  0.033356  0.005522  ...  0.097502  0.009506  0.217169  0.100670   \n",
      "2   0.049401  0.033356  0.005522  ...  0.097502  0.009506  0.217169  0.100670   \n",
      "3   0.113919  0.058509  0.092570  ...  0.305692  0.159653  0.193317  0.061736   \n",
      "4   0.113919  0.058509  0.092570  ...  0.305692  0.159653  0.193317  0.061736   \n",
      "5   0.113919  0.058509  0.092570  ...  0.305692  0.159653  0.193317  0.061736   \n",
      "6   0.067231  0.038613  0.015778  ...  0.109386  0.024217  0.280625  0.012974   \n",
      "7   0.067231  0.038613  0.015778  ...  0.109386  0.024217  0.280625  0.012974   \n",
      "8   0.067231  0.038613  0.015778  ...  0.109386  0.024217  0.280625  0.012974   \n",
      "9   0.100705  0.032141  0.100002  ...  0.038581  0.032267  0.026153  0.010779   \n",
      "10  0.100705  0.032141  0.100002  ...  0.038581  0.032267  0.026153  0.010779   \n",
      "11  0.100705  0.032141  0.100002  ...  0.038581  0.032267  0.026153  0.010779   \n",
      "12  0.117249  0.035235  0.101070  ...  0.021259  0.027071  0.029206  0.006495   \n",
      "13  0.117249  0.035235  0.101070  ...  0.021259  0.027071  0.029206  0.006495   \n",
      "14  0.117249  0.035235  0.101070  ...  0.021259  0.027071  0.029206  0.006495   \n",
      "15  0.044664  0.004796  0.052219  ...  0.084273  0.042964  0.128076  0.037557   \n",
      "16  0.044664  0.004796  0.052219  ...  0.084273  0.042964  0.128076  0.037557   \n",
      "17  0.044664  0.004796  0.052219  ...  0.084273  0.042964  0.128076  0.037557   \n",
      "18  0.042506  0.036583  0.025745  ...  0.042711  0.029550  0.045899  0.034033   \n",
      "19  0.042506  0.036583  0.025745  ...  0.042711  0.029550  0.045899  0.034033   \n",
      "20  0.042506  0.036583  0.025745  ...  0.042711  0.029550  0.045899  0.034033   \n",
      "21  0.138786  0.070177  0.140488  ...  0.084506  0.083272  0.644688  0.131429   \n",
      "22  0.138786  0.070177  0.140488  ...  0.084506  0.083272  0.644688  0.131429   \n",
      "23  0.138786  0.070177  0.140488  ...  0.084506  0.083272  0.644688  0.131429   \n",
      "24  0.044836  0.000262  0.026874  ...  0.088175  0.058499  0.432515  0.012727   \n",
      "25  0.044836  0.000262  0.026874  ...  0.088175  0.058499  0.432515  0.012727   \n",
      "26  0.044836  0.000262  0.026874  ...  0.088175  0.058499  0.432515  0.012727   \n",
      "27  0.189330  0.081771  0.207710  ...  0.282464  0.028169  0.433739  0.119059   \n",
      "28  0.189330  0.081771  0.207710  ...  0.282464  0.028169  0.433739  0.119059   \n",
      "29  0.189330  0.081771  0.207710  ...  0.282464  0.028169  0.433739  0.119059   \n",
      "\n",
      "          18        19        20        21  22  binary_label  \n",
      "0   1.503086  0.030362  0.291363  0.210082   0             1  \n",
      "1   1.503086  0.030362  0.291363  0.210082   1             1  \n",
      "2   1.503086  0.030362  0.291363  0.210082   2             0  \n",
      "3   1.248600  0.206529  0.351693  0.105346   0             1  \n",
      "4   1.248600  0.206529  0.351693  0.105346   1             1  \n",
      "5   1.248600  0.206529  0.351693  0.105346   2             0  \n",
      "6   0.545858  0.075520  0.304185  0.049345   0             1  \n",
      "7   0.545858  0.075520  0.304185  0.049345   1             1  \n",
      "8   0.545858  0.075520  0.304185  0.049345   2             0  \n",
      "9   0.334111  0.105626  0.137693  0.031455   0             1  \n",
      "10  0.334111  0.105626  0.137693  0.031455   1             1  \n",
      "11  0.334111  0.105626  0.137693  0.031455   2             0  \n",
      "12  0.165401  0.081549  0.091768  0.030546   0             1  \n",
      "13  0.165401  0.081549  0.091768  0.030546   1             1  \n",
      "14  0.165401  0.081549  0.091768  0.030546   2             0  \n",
      "15  0.775643  0.162126  0.209275  0.105691   0             1  \n",
      "16  0.775643  0.162126  0.209275  0.105691   1             1  \n",
      "17  0.775643  0.162126  0.209275  0.105691   2             1  \n",
      "18  0.179981  0.086489  0.147702  0.050502   0             1  \n",
      "19  0.179981  0.086489  0.147702  0.050502   1             0  \n",
      "20  0.179981  0.086489  0.147702  0.050502   2             0  \n",
      "21  1.235154  0.137911  0.629805  0.234235   0             1  \n",
      "22  1.235154  0.137911  0.629805  0.234235   1             1  \n",
      "23  1.235154  0.137911  0.629805  0.234235   2             0  \n",
      "24  0.509642  0.125125  0.397542  0.025845   0             1  \n",
      "25  0.509642  0.125125  0.397542  0.025845   1             1  \n",
      "26  0.509642  0.125125  0.397542  0.025845   2             0  \n",
      "27  1.540188  0.074895  0.410333  0.200164   0             0  \n",
      "28  1.540188  0.074895  0.410333  0.200164   1             0  \n",
      "29  1.540188  0.074895  0.410333  0.200164   2             0  \n",
      "\n",
      "[30 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "new_aesthetic_evaluation_X, new_aesthetic_evaluation_y, new_aesthetic_evaluation_data, _ = sbc_reduction(aesthetic_evaluation_X, aesthetic_evaluation_y, aesthetic_evaluation_mapping, h=1)\n",
    "print(\"old aesthetic evaluation data:\")\n",
    "print(aesthetic_evaluation_data)\n",
    "print(\"new aesthetic evaluation data:\")\n",
    "print(new_aesthetic_evaluation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply a linear two class classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 1 0 1 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "logistic.solver = 'liblinear'\n",
    "logistic.penalty = 'l1'\n",
    "logistic.fit(new_X, new_y.values.ravel())  # convert y to 1d array\n",
    "pred_sbc_y = logistic.predict(new_X)\n",
    "print(pred_sbc_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "logistic.fit(new_aesthetic_evaluation_X, new_aesthetic_evaluation_y.values.ravel()) \n",
    "pred_sbc_y_aesthetic = logistic.predict(new_aesthetic_evaluation_X)\n",
    "print(pred_sbc_y_aesthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to classify an example\n",
    "- classify all its replicas\n",
    "- get a sequence of $K-1$ labels\n",
    "- from this sequence, infer the class of the original example following the rule:\n",
    "    (for $K=3$)\n",
    "    - if both are $C_1$, then the class is $C_1$\n",
    "    - if one is $C_1$ and the other is $C_2$, then the class is $C_2$\n",
    "    - if both are $C_2$, then the class is $C_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all labels: [[1 0 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 1]]\n",
      "final labels (before mapping):  [1 2 2 3]\n",
      "mapping:  {0: 'a', 1: 'b', 2: 'c', 3: 'd'}\n",
      "predicted labels:  0    b\n",
      "1    c\n",
      "2    c\n",
      "3    d\n",
      "dtype: object\n",
      "real labels:  ['a' 'b' 'c' 'd']\n",
      "accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "def sbc_classif(og_y, pred_sbc_y, mapping):\n",
    "    K = len(np.unique(og_y)) # num of classes\n",
    "    s = K-1 # num of hyperplanes / replicas\n",
    "    \n",
    "    # get classification of all replicas of each point\n",
    "    all_labels = [pred_sbc_y[i:i + s] for i in range(0, len(pred_sbc_y), s)]\n",
    "    all_labels = np.array(all_labels)\n",
    "    print(\"all labels:\", all_labels)\n",
    "    \n",
    "    # get the class of the point\n",
    "    # if all replicas are 0, then the class is 0\n",
    "    # if one is 1 and the rest are 0, then the class is 1\n",
    "    # if two are 1 and the rest are 0, then the class is 2\n",
    "    # ...\n",
    "    # if all replicas are 1, then the class is K\n",
    "    final_labels = np.sum(all_labels, axis=1)\n",
    "    print(\"final labels (before mapping): \", final_labels)\n",
    "    \n",
    "    # convert back to original labels if mapping is provided\n",
    "    if mapping is not None:\n",
    "        print(\"mapping: \", mapping)\n",
    "        final_labels = pd.Series(final_labels).map(mapping)\n",
    "    \n",
    "    return final_labels\n",
    "\n",
    "\n",
    "final_labels = sbc_classif(y, pred_sbc_y, mapping)\n",
    "print(\"predicted labels: \", final_labels)\n",
    "accuracy = accuracy_score(y, final_labels)\n",
    "print(\"real labels: \", y.values)\n",
    "print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all labels: [[1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 0 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [0 0 0]]\n",
      "final labels (before mapping):  [2 2 2 1 2 2 2 2 2 0]\n",
      "mapping:  {0: 'Poor', 1: 'Fair', 2: 'Good', 3: 'Excellent'}\n",
      "predicted labels aesthetic:  0    Good\n",
      "1    Good\n",
      "2    Good\n",
      "3    Fair\n",
      "4    Good\n",
      "5    Good\n",
      "6    Good\n",
      "7    Good\n",
      "8    Good\n",
      "9    Poor\n",
      "dtype: object\n",
      "real labels aesthetic:  ['Good' 'Good' 'Good' 'Good' 'Good' 'Excellent' 'Fair' 'Good' 'Good'\n",
      " 'Poor']\n",
      "accuracy aesthetic:  0.7\n"
     ]
    }
   ],
   "source": [
    "final_labels_aesthetic = sbc_classif(aesthetic_evaluation_y, pred_sbc_y_aesthetic, aesthetic_evaluation_mapping)\n",
    "print(\"predicted labels aesthetic: \", final_labels_aesthetic)\n",
    "accuracy_aesthetic = accuracy_score(aesthetic_evaluation_y, final_labels_aesthetic)\n",
    "print(\"real labels aesthetic: \", aesthetic_evaluation_y.values)\n",
    "print(\"accuracy aesthetic: \", accuracy_aesthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped y:  0    a\n",
      "1    b\n",
      "2    c\n",
      "3    d\n",
      "dtype: object\n",
      "Mapped y_2:  0    0\n",
      "1    1\n",
      "2    2\n",
      "3    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def apply_mapping(y, mapping):\n",
    "    if (y.dtype == 'int'):\n",
    "        return pd.Series(y).map(mapping)\n",
    "    else:\n",
    "        return pd.Series(y.map(lambda v: {v_:k_ for k_, v_ in mapping.items()}[v]))\n",
    "\n",
    "y = [0, 1, 2, 3]\n",
    "y = pd.Series(y)\n",
    "mapped_y = apply_mapping(y, mapping)\n",
    "print(\"Mapped y: \", mapped_y)\n",
    "\n",
    "y_2 = ['a', 'b', 'c', 'd']\n",
    "y_2 = pd.Series(y_2)\n",
    "mapped_y_2 = apply_mapping(y_2, mapping)\n",
    "print(\"Mapped y_2: \", mapped_y_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
