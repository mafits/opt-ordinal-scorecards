{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced Memory Multi-Pass (RMMP) Algorithm\n",
    "*\"Algorithms for Sparse Linear Classifiers in the Massive Data Setting\"*, 2008 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**modified shooting algorithm**\n",
    "\n",
    "- goal: $\\beta$ that satisfies $ \\max_\\beta (\\beta ^ T \\Psi \\beta + \\beta ^ T \\theta - \\gamma ||\\beta||_1) $\n",
    "\n",
    "- \"The vector $\\Omega$ in the algorithm is defined as $\\Omega = 2 \\Psi ' \\beta + \\theta$, where $\\Psi '$ is the matrix $\\Psi$ with its diagonal entries set to zero. This vector is related to the gradient of the differentiable part of the objective function and consequently can be used for optimality checking.\"\n",
    "\n",
    "- \"While one can think of numerous stopping criteria for the algorithm, in this paper we stop when successive iterates are sufficiently close to each other (relatively, and with respect to the L2). \n",
    "More precisely, we declare convergence whenever\n",
    "$||\\beta_i - \\beta_{i-1}||_2 / ||\\beta_{i-1}||_2$\n",
    "is less than some user specified tolerance. Note that $\\beta_i$ is the parameter vector at iteration $i$, which is obtained after cycling through and updating all $d$ components once.\n",
    "\n",
    "\n",
    "![modified shooting pseudocode](images/shooting-pseudocode.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ||Bi - Bi-1|| / ||Bi-1|| < tolerance\n",
    "def has_converged(new_parameters, parameters, tolerance=1e-6):\n",
    "    return (np.linalg.norm(new_parameters - parameters) / np.linalg.norm(parameters)) < tolerance\n",
    "\n",
    "\n",
    "def modified_shooting(parameters, d, vector_stats, matrix_stats, tolerance=1e-6):\n",
    "    # initialize new_parameters\n",
    "    new_parameters = np.zeros_like(parameters)\n",
    "    \n",
    "    # create a new matrix_stats with the diagonals set to 0\n",
    "    new_matrix_stats = matrix_stats.copy()\n",
    "    new_matrix_stats[np.diag_indices(d)] = 0\n",
    "    \n",
    "    # initialize gradient_vector\n",
    "    gradient_vector = np.zeros(d)\n",
    "    \n",
    "    while True:\n",
    "        for j in range(d):\n",
    "            # update new_parameters based on tolerance\n",
    "            if(abs(gradient_vector[j]  <= tolerance)):\n",
    "                new_parameters[j] = 0\n",
    "            elif gradient_vector[j] > tolerance:\n",
    "                new_parameters[j] = (tolerance - gradient_vector[j]) / (2 * matrix_stats[j, j])\n",
    "            elif gradient_vector[j] < -tolerance:\n",
    "                new_parameters[j] = (- tolerance - gradient_vector[j]) / (2 * matrix_stats[j, j])\n",
    "            \n",
    "            # update gradient_vector\n",
    "            gradient_vector = 2 * new_matrix_stats @ new_parameters + vector_stats\n",
    "        \n",
    "        # check for convergence\n",
    "        if has_converged(new_parameters, parameters, tolerance):\n",
    "            break\n",
    "        \n",
    "        # update parameters\n",
    "        parameters = new_parameters.copy()\n",
    "    return new_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the reduced memory multi-pass (RMMP) algorithm**\n",
    "![rmmp pseudocode](<images/rmmp-pseudocode.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for $y_i=1$\n",
    "\n",
    "![ai for yi=1](images/ai-for-yi-1.png)\n",
    "\n",
    "![bi for yi=1](images/bi-for-yi-1.png)\n",
    "\n",
    "for $y_i=0$\n",
    "\n",
    "![ai and bi for yi=0](images/ai-bi-for-yi-0.png)\n",
    "\n",
    "$c$^ $= \\beta_{i-1}^T x_i$\n",
    "\n",
    "$\\Phi$ is the link function, either logistic or probit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic link function\n",
    "def logit_model(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def logit_model_derivative(x):\n",
    "    return logit_model(x) * (1 - logit_model(x))\n",
    "\n",
    "def logit_model_double_derivative(x):\n",
    "    return logit_model_derivative(x) * (1 - 2 * logit_model(x))\n",
    "\n",
    "\n",
    "# quadratic approximation to term likelihood at parameters\n",
    "def quad_approximation(xi, yi, parameters, d):\n",
    "    # initialize ai and bi\n",
    "    ai = np.zeros((d, d))\n",
    "    bi = np.zeros(d)\n",
    "    \n",
    "    c = parameters.T @ xi\n",
    "    \n",
    "    if yi == 1:\n",
    "        ai = 0.5 * (logit_model_double_derivative(c) / logit_model(c) \n",
    "                    - (logit_model_derivative(c) / logit_model(c)) **2\n",
    "                    )\n",
    "        bi = logit_model_derivative(c) / logit_model(c) - c * ai\n",
    "    \n",
    "    elif yi == 0:\n",
    "        ai = 0.5 * (logit_model_double_derivative(c) / (1 - logit_model(c)) - (logit_model_derivative(c) / (1 - logit_model(c)))**2)\n",
    "        bi = logit_model_derivative(c) / (1 - logit_model(c)) + c * ai\n",
    "    \n",
    "    return ai, bi\n",
    "\n",
    "\n",
    "\n",
    "def rmmp(X, y, tolerance=1e-6):\n",
    "    # number of examples and dimension\n",
    "    t, d = X.shape\n",
    "    \n",
    "    parameters = np.zeros(d) # parameters of the regression model\n",
    "    active_set = set() # components that are either non-zero and optimal or not optimal\n",
    "    counter = 1 \n",
    "    gradient_vector = np.zeros(d)\n",
    "    \n",
    "    while True:\n",
    "        vector_stats = np.zeros(d)\n",
    "        matrix_stats = np.zeros((d, d)) \n",
    "        \n",
    "        for i in range(t):\n",
    "            # get ith observation (xi, yi)\n",
    "            xi = X[i].reshape(-1, 1)\n",
    "            yi = y[i]\n",
    "            \n",
    "            ai, bi = quad_approximation(xi, yi, parameters, d)\n",
    "            \n",
    "            matrix_stats += ai * (xi @ xi.T)\n",
    "            vector_stats += bi * xi\n",
    "            \n",
    "            new_matrix_stats = matrix_stats.copy()\n",
    "            new_matrix_stats[np.diag_indices(d)] = 0\n",
    "            gradient_vector = 2 * new_matrix_stats @ new_parameters + vector_stats\n",
    "            \n",
    "        new_parameters = modified_shooting(parameters, d, vector_stats, matrix_stats, tolerance)\n",
    "        active_set = {j for j in range(d) if gradient_vector[j] >= tolerance}\n",
    "        \n",
    "        if has_converged(new_parameters, parameters):\n",
    "            break\n",
    "        \n",
    "        parameters = new_parameters\n",
    "        counter += 1\n",
    "    \n",
    "    return parameters, active_set\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
