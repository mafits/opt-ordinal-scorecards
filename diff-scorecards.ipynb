{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Differential Scorecards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# datasets\n",
    "from sklearn.datasets import load_iris\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from io import StringIO\n",
    "\n",
    "# discretization\n",
    "from libraries.caimcaim import CAIMD # https://github.com/airysen/caimcaim/blob/master/caimcaim/caimcaim.py\n",
    "\n",
    "# cv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# objective function\n",
    "from scipy.optimize import least_squares\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# regularization\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **binary data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### benchmark datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iris**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = load_iris()\n",
    "iris_X = pd.DataFrame(iris_data.data)\n",
    "iris_y = pd.DataFrame(iris_data.target)\n",
    "\n",
    "print(\"num observations: \", iris_y.count())\n",
    "print(\"target distribution: \", iris_y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**adult**: predict whether annual income of an individual exceeds $50K/yr based on census data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "adult_data = fetch_ucirepo(id=2) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "adult_X = adult_data.data.features \n",
    "adult_y = adult_data.data.targets\n",
    "adult_y.loc[:,'income'] = adult_y['income'].map({'>50K': 1, '<=50K': 0})\n",
    "  \n",
    "# metadata \n",
    "# print(adult_data.metadata) \n",
    "  \n",
    "# variable information \n",
    "# print(adult_data.variables) \n",
    "\n",
    "print(\"num observations: \", adult_y.count())\n",
    "print(\"target distribution: \", adult_y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mammo**: discrimination of benign and malignant mammographic masses based on BI-RADS attributes and the patient's age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "mammo_data = fetch_ucirepo(id=161) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "mammo_X = mammo_data.data.features \n",
    "mammo_y = mammo_data.data.targets \n",
    "\n",
    "# drop rows with nulls\n",
    "mammo_combined = pd.concat([mammo_X, mammo_y], axis=1)\n",
    "print(\"num rows with nulls: \", mammo_combined.isnull().sum().sum())\n",
    "mammo_combined = mammo_combined.dropna()\n",
    "mammo_combined = mammo_combined.reset_index(drop=True)\n",
    "mammo_X = mammo_combined.iloc[:, :-1]\n",
    "mammo_y = mammo_combined.iloc[:, -1]\n",
    "\n",
    "\n",
    "# metadata \n",
    "# print(mammo_data.metadata) \n",
    "  \n",
    "# variable information \n",
    "# print(mammo_data.variables) \n",
    "\n",
    "print(\"num observations: \", mammo_y.count())\n",
    "print(\"target distribution: \", mammo_y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mushroom**: mushrooms described in terms of physical characteristics; classification: poisonous or edible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "mushroom_data = fetch_ucirepo(id=73) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "mushroom_X = mushroom_data.data.features \n",
    "mushroom_y = mushroom_data.data.targets \n",
    "mushroom_y.loc[:, 'poisonous'] = mushroom_y['poisonous'].map({'p': 1, 'e': 0})\n",
    "  \n",
    "# metadata \n",
    "# print(mushroom_data.metadata) \n",
    "  \n",
    "# variable information \n",
    "# print(mushroom_data.variables) \n",
    "\n",
    "print(\"num observations: \", mushroom_y.count())\n",
    "print(\"target distribution: \", mushroom_y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spambase**: classifying Email as Spam or Non-Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "spambase_data = fetch_ucirepo(id=94) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "spambase_X = spambase_data.data.features \n",
    "spambase_y = spambase_data.data.targets \n",
    "  \n",
    "# metadata \n",
    "# print(spambase_data.metadata) \n",
    "  \n",
    "# variable information \n",
    "# print(spambase_data.variables) \n",
    "\n",
    "print(\"num observations: \", spambase_y.count())\n",
    "print(\"target distribution: \", spambase_y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**telemarketing**: set of possible advertisements on Internet pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from file. target is last column\n",
    "telemarketing_data = pd.read_csv('datasets/internet+advertisements/ad.data', dtype=str)\n",
    "telemarketing_X = telemarketing_data.iloc[:, :-1]\n",
    "telemarketing_y = telemarketing_data.iloc[:, -1]\n",
    "\n",
    "telemarketing_y = telemarketing_y.map({'nonad.': 0, 'ad.': 1})\n",
    "\n",
    "print(\"num observations: \", telemarketing_y.count())\n",
    "print(\"target distribution: \", telemarketing_y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sleep apnea**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_apnea_data = pd.read_csv('datasets/bdsp_psg_master_20231101.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**appendicitis**: https://sci2s.ugr.es/keel/dataset.php?cod=183#sub2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/appendicitis.dat', \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "data_start_ind = lines.index(\"@data\\n\") + 1\n",
    "app_data = lines[data_start_ind:]\n",
    "app_data = pd.read_csv(StringIO(\"\".join(app_data)), header=None)\n",
    "\n",
    "app_data.columns = [\"At1\", \"At2\", \"At3\", \"At4\", \"At5\", \"At6\", \"At7\", \"Class\"]\n",
    "\n",
    "app_X = app_data.iloc[:, :-1]\n",
    "app_y = app_data.iloc[:, -1]\n",
    "\n",
    "print(\"num observations: \", app_y.count())\n",
    "print(\"target distribution: \", app_y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **discretization thresholds**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretize using CAIM\n",
    "def discretize_caim_df(data, X, y):\n",
    "    caim = CAIMD()\n",
    "    X_disc_caim = caim.fit_transform(X, y) # fit() and transform()\n",
    "    \n",
    "    print(\"\\nCut-off points: \", caim.split_scheme)\n",
    "    print(\"Number of bins: \", end=\"\")\n",
    "    for i, (key, value) in enumerate(caim.split_scheme.items()):\n",
    "        if i == len(caim.split_scheme) - 1:\n",
    "            print(f\" {key}: {len(value)+1}\", end=\"\")\n",
    "        else:\n",
    "            print(f\" {key}: {len(value)+1}\", end=\",\")\n",
    "    print()\n",
    "    \n",
    "    X_disc_caim = pd.DataFrame(X_disc_caim, columns=X.columns).astype(int) # convert to pandas dataframe and int\n",
    "        \n",
    "    return X_disc_caim\n",
    "\n",
    "''' \n",
    "prints of fit() method: Categorical list_of_(indicies)_categorical_features\n",
    "    # feature_index  GLOBAL CAIM  best_caim_value \n",
    "in the returning dataframe:\\\n",
    "    - columns represent the original features\n",
    "    - rows represent each instance\n",
    "    - values are the bin number each instance belongs to (starting from 0)\n",
    "'''\n",
    "\n",
    "def discretize_caim(X, cols, y):\n",
    "    caim = CAIMD()\n",
    "    X_disc_caim = caim.fit_transform(X, y) # fit() and transform()\n",
    "    # get thresholds from caim.split_scheme (dict with column index : thresholds)\n",
    "    # transform all values to floats\n",
    "    # and keys with column indexes to column names\n",
    "    thresholds = {cols[i]: [float(val) for val in value] for i, (key, value) in enumerate(caim.split_scheme.items())}\n",
    "    return thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_thresholds_caim = discretize_caim(app_X, app_X.columns, app_y)\n",
    "print(\"\\nthresholds \", app_thresholds_caim)\n",
    "\n",
    "print(\"num of bins: \")\n",
    "for i, (key, value) in enumerate(app_thresholds_caim.items()):\n",
    "        print(f\"  {key}: {len(value)+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "infinitesimal bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretize using infinitesimal bins:\n",
    "# thresholds are the points in between 2 consecutive values in the sorted list\n",
    "\n",
    "def discretize_infbins(X, cols):\n",
    "    infbins_thresholds = {}\n",
    "    for col in cols:\n",
    "        # sort unique values\n",
    "        sorted_col = np.unique(X[col])\n",
    "        # get thresholds\n",
    "        thresholds = (sorted_col[:-1] + sorted_col[1:]) / 2\n",
    "        infbins_thresholds[col] = thresholds.tolist()\n",
    "\n",
    "    return infbins_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_thresholds_infbins = discretize_infbins(app_X, app_X.columns)\n",
    "print(\"thresholds \", app_thresholds_infbins)\n",
    "print(\"num of bins: \")\n",
    "for i, (key, value) in enumerate(app_thresholds_infbins.items()):\n",
    "        print(f\"  {key}: {len(value)+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### discretized version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num of columns in the new df = (num thresholds + 1) * num features = num bins * num features\n",
    "\n",
    "2 methods\n",
    "- 1 out of k: 1 if the value is in the bin, 0 otherwise\n",
    "- differential coding: 1 from bin 1 until bin where the value is in, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bins(thresholds, values):\n",
    "    bins = np.digitize(values, thresholds)\n",
    "    return bins\n",
    "    # list of bin number for each row\n",
    "\n",
    "\n",
    "def disc_1_out_of_k(X, cols, thresholds):\n",
    "    disc_df = []\n",
    "    for col in cols:\n",
    "        bins = get_bins(thresholds[col], X[col]) # gets bin number of each row\n",
    "        bins_df = pd.get_dummies(bins, prefix=f'feat{col}-bin', prefix_sep='').astype(int) # one hot encoding\n",
    "        #for i in range(1, len(thresholds[col]) + 1):\n",
    "        #    if f'feat{col}-bin_{i}' not in bins_df.columns:\n",
    "        #        bins_df[f'feat{col}-bin{i}'] = 0\n",
    "        bins_df = bins_df.drop(columns=f'feat{col}-bin0', errors='ignore')\n",
    "        disc_df.append(bins_df)    \n",
    "    return pd.concat(disc_df, axis=1)\n",
    "\n",
    "\n",
    "def disc_diff_coding(X, cols, thresholds):\n",
    "    bin_dfs = []\n",
    "    for col in cols:\n",
    "        bins = get_bins(thresholds[col], X[col]) # gets bin number of each row\n",
    "        num_bins = len(thresholds[col]) + 1\n",
    "        bin_df = pd.DataFrame(0, index=X.index, columns=[f'feat{col}-bin{i}' for i in range(1, num_bins)])\n",
    "        for i in range(1, num_bins):\n",
    "            bin_df[f'feat{col}-bin{i}'] = (bins >= i).astype(int)\n",
    "        bin_dfs.append(bin_df)\n",
    "    return pd.concat(bin_dfs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_X_disc_caim_1outofk = disc_1_out_of_k(app_X, app_X.columns, app_thresholds_caim)\n",
    "disc_app_X = app_X_disc_caim_1outofk\n",
    "app_X_disc_caim_1outofk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_X_disc_caim_diffcod = disc_diff_coding(app_X, app_X.columns, app_thresholds_caim)\n",
    "app_X_disc_caim_diffcod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_X_disc_infbins_1outofk = disc_1_out_of_k(app_X, app_X.columns, app_thresholds_infbins)\n",
    "app_X_disc_infbins_1outofk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_X_disc_infbins_diffcod = disc_diff_coding(app_X, app_X.columns, app_thresholds_infbins)\n",
    "app_X_disc_infbins_diffcod.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_score(model, X, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        scores.append(mean_squared_error(y_test, y_pred))\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0.01, 0.1, 0.4, 0.6, 0.9, 0.99]\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "elastic_net = ElasticNet()\n",
    "\n",
    "param_grid = {'alpha': alpha}\n",
    "\n",
    "def grid_search(model, X, y, param_grid, cv=10):\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv)\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Least Squares (RSS)\n",
    "- Maximum Likelihood (GLM with binomial response and logit link function)\n",
    "- margin maximization (linear SVM)\n",
    "\n",
    "LS and ML objective functions were regularized by means of an elastic net, with the a parameter\n",
    "being determined by grid search over the range {0.01, 0.1, 0.4, 0.6, 0.9, 0.99}.\n",
    "\n",
    "The linear SVM parameter, C, was determined by grid-search over the range {2^−10, 2^−9, . . . , 2^9, 2^10}.\n",
    "All results involving SVM were obtained through the use of the library LIBSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(model, disc_X, y):\n",
    "    model.fit(disc_X, y)\n",
    "    weights = model.coef_[0]\n",
    "    feature_names = disc_X.columns\n",
    "    weights_df = pd.DataFrame({'Feature': feature_names, 'Weight': weights})\n",
    "    return weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSS\n",
    "grid_search_rss = grid_search(elastic_net, disc_app_X, app_y, param_grid)\n",
    "best_model_rss = grid_search_rss.best_estimator_\n",
    "weights_rss = get_weights(best_model_rss, disc_app_X, app_y)\n",
    "print(\"RSS weights:\\n\", weights_rss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum likelihood (GLM with binomial response and logit link function)\n",
    "logistic = LogisticRegression()\n",
    "weights_ml = get_weights(logistic, disc_app_X, app_y)\n",
    "print(\"ML weights:\\n\", weights_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# margin maximization (liner SVM)\n",
    "param_grid = {\n",
    "    'C': [2**i for i in range(-10, 11)]\n",
    "}\n",
    "svm = SVC(kernel='linear')\n",
    "grid_search_svm = grid_search(svm, disc_app_X, app_y, param_grid)\n",
    "best_model_svm = grid_search_svm.best_estimator_\n",
    "weights_svm = get_weights(best_model_svm, disc_app_X, app_y)\n",
    "print(\"SVM weights:\\n\", weights_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorecard(data, X, y, disc_scheme_method, disc_version_method, obj_function, regularization):\n",
    "    disc_scheme = {}\n",
    "    if(disc_scheme_method == 'caim'):\n",
    "        disc_scheme = discretize_caim(X, X.columns, y)\n",
    "    elif(disc_scheme_method == 'infbins'):\n",
    "        disc_scheme = discretize_infbins(X, X.columns)\n",
    "        \n",
    "    disc_version = pd.DataFrame()\n",
    "    if(disc_version_method == '1outofk'):\n",
    "        disc_version = disc_1_out_of_k(X, X.columns, disc_scheme)\n",
    "    elif(disc_version_method == 'diffcod'):\n",
    "        disc_version = disc_diff_coding(X, X.columns, disc_scheme)\n",
    "    \n",
    "    model = None\n",
    "    param_grid = {}\n",
    "    if(obj_function == 'RSS'):\n",
    "        model = ElasticNet()\n",
    "        param_grid = {'alpha': [0.01, 0.1, 0.4, 0.6, 0.9, 0.99]}\n",
    "    elif(obj_function == 'ML'):\n",
    "        model = LogisticRegression()\n",
    "        param_grid = {'alpha': [0.01, 0.1, 0.4, 0.6, 0.9, 0.99]}\n",
    "    elif(obj_function == 'SVM'):\n",
    "        model = SVC(kernel='linear')\n",
    "        param_grid = {'C': [2**i for i in range(-10, 11)]}\n",
    "    \n",
    "    grid_search_model = grid_search(model, disc_version, y, param_grid)\n",
    "\n",
    "    best_model = grid_search_model.best_estimator_\n",
    "    weights = get_weights(best_model, disc_version, y)\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_weights = scorecard(app_data, app_X, app_y, 'caim', '1outofk', 'SVM', 'ridge')\n",
    "print(\"app weights: \", app_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ordinal data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aesthetic_evaluation_data = pd.read_csv('datasets/aesthetic_evaluation_data.csv')\n",
    "aesthetic_evaluation_data = aesthetic_evaluation_data.drop(columns=['Image Filename','Author','Objective Evaluation'])\n",
    "aesthetic_evaluation_X = aesthetic_evaluation_data.drop(columns='Subjective Evaluation')\n",
    "aesthetic_evaluation_y = aesthetic_evaluation_data['Subjective Evaluation']\n",
    "\n",
    "aesthetic_evaluation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = aesthetic_evaluation_data[['sX2L Value','sX2a Value','sX2b Value','sX2Lab Value','sEMDL Value','sEMDa Value','sEMDb Value','sEMDLab Value']]\n",
    "a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aesthetic_evaluation_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aesthetic_evaluation_y.hist()\n",
    "plt.xlabel('Subjective Evaluation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Subjective Evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SBC (single binary classifier) reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
